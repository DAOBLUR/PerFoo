# -*- coding: utf-8 -*-
"""PerFoo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rp82AOu3krte7kdr1lyMNZxsYdbYjjqz
"""

!pip install transformers
#!pip install transformers[all-deps]

from transformers import RobertaTokenizer, RobertaForQuestionAnswering, AdamW, get_linear_schedule_with_warmup
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from textwrap import wrap

# initialize
RANDOM_SEED = 42
MAX_LEN = 200
BATCH_SIZE = 16
DATASET_PATH = '/content/drive/My Drive/UNI/7/SoftwareModels/PerFoo/project/clean-reviews.csv'
DATASET_PATH = '/content/drive/My Drive/UNI/7/SoftwareModels/PerFoo/project/data.csv'
NCLASSES = 2

np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

# load dataset
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv(DATASET_PATH)
df = df[0:10000]

print(df.head())
print(df.shape)
print("\n".join(wrap(df['review'][200])))

# TOKENIZACION
from transformers import BertModel, BertTokenizer

PRE_TRAINED_MODEL_NAME = "dccuchile/bert-base-spanish-wwm-uncased" #"IIC/roberta-base-spanish-sqac"
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
#tokenizer = RobertaTokenizer.from_pretrained("IIC/roberta-base-spanish-sqac")

# tokenization test
sample = 'Muy buena presentaci贸n  y servicio sin embargo exageradamente costoso y no se informa ennla carta lis precios,Muy buena presentaci贸n y servicio'
tokens = tokenizer.tokenize(sample)
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print('Frase: ', sample)
print('Tokens: ', tokens)
print('Numerics tokens: ', token_ids)

#!pip install nltk
import nltk
import nltk.data
from nltk.tokenize import sent_tokenize

nltk.download('punkt')

sample = "Muy buena presentaci贸n y servicio sin embargo exageradamente costoso y no se informa ennla carta lis precios,Muy buena presentaci贸n y servicio"
sample = "Hola amigos. Gracias por ver este video. Saludos"
tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')
print(tokenizer.tokenize(sample))

"""-------------------------------------"""

# codificatio for BETO
sample = 'Muy buena presentaci贸n  y servicio sin embargo exageradamente costoso y no se informa ennla carta lis precios,Muy buena presentaci贸n y servicio'

encoding = tokenizer.encode_plus(
    sample,
    max_length = 100,
    truncation = True,
    add_special_tokens = True,
    return_token_type_ids = False,
    pad_to_max_length = True,
    return_attention_mask = True,
    return_tensors = 'pt'
)

encoding.keys()
print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))
print(encoding['input_ids'][0])
print(encoding['attention_mask'][0])

# models

class BETOSentimentClassifier(nn.Module):

  def __init__(self, n_classes):
    super(BERTSentimentClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.drop = nn.Dropout(p=0.3)
    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)


  def forward(self, input_ids, attention_mask):
    _, cls_output = self.bert(
        input_ids = input_ids,
        attention_mask = attention_mask
    )
    drop_output = self.drop(cls_output)
    output = self.linear(drop_output)
    return output

print(df['review'])

from wordcloud import WordCloud

wordcloud = WordCloud().generate(' '.join(df['review']))

import matplotlib.pyplot as plt
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()